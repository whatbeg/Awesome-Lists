NIPS 2017

* Breaking the Nonsmooth Barrier: A Scalable Parallel Method for Composite Optimization
Fabian Pedregosa (UC Berkeley / ETH Zurich) · Rémi Leblond (INRIA) · Simon Lacoste-Julien (Université de Montréal)

* Parametric Simplex Method for Sparse Learning
Haotian Pang (Princeton University) · Tuo Zhao (Georgia Tech) · Han Liu (Tencent AI Lab) · Robert J Vanderbei (Princeton University)

* Group Sparse Additive Machine
Hong Chen (University of Pittsburgh) · Xiaoqian Wang (University of Pittsburgh) · Heng Huang (Electrical and Computer Engineering University of Pittsburgh)

* Learning with Average Top-k Loss
Yanbo Fan (NLPR, CASIA) · Siwei Lyu (SUNY at Albany) · Yiming Ying (State University of New York at Albany) · Baogang Hu (Chinese Academy of Sciences)

* Dykstra's Algorithm, ADMM, and Coordinate Descent: Connections, Insights, and Extensions
Ryan Tibshirani (Carnegie Mellon University)

* Doubly Accelerated Stochastic Variance Reduced Dual Averaging Method for Regularized Empirical Risk Minimization
Tomoya Murata (NTT DATA Mathematical Systems Inc.) · Taiji Suzuki ()

* Coded Distributed Computing for Inverse Problems
Yaoqing Yang (Carnegie Mellon University) · Pulkit Grover (CMU) · Soummya Kar (Carnegie Mellon University)

* Compression-aware Training of Deep Neural Networks
Jose Alvarez (TRI) · Mathieu Salzmann (EPFL)

* Self-Normalizing Neural Networks
Günter Klambauer (LIT AI Lab / University Linz) · Thomas Unterthiner (LIT AI Lab / University Linz) · Andreas Mayr (LIT AI Lab / University Linz) · Sepp Hochreiter (LIT AI Lab / University Linz)

* Active Bias: Training a More Accurate Neural Network by Emphasizing High Variance Samples
Haw-Shiuan Chang (UMass, Amherst) · Andrew McCallum (UMass Amherst) · Erik Learned-Miller (UMass Amherst)

* Gradient Descent Can Take Exponential Time to Escape Saddle Points
Simon Du (Carnegie Mellon University) · Chi Jin (UC Berkeley) · Jason D Lee (USC) · Michael Jordan (UC Berkeley) · Aarti Singh (CMU) · Barnabas Poczos (Carnegie Mellon University)

* Integration Methods and Optimization Algorithms
Damien Scieur (INRIA - ENS) · Vincent Roulet (INRIA / ENS Ulm) · Francis Bach (Inria) · Alexandre d'Aspremont (CNRS - Ecole Normale Supérieure)

* Sharpness, Restart and Acceleration
Vincent Roulet (INRIA / ENS Ulm) · Alexandre d'Aspremont (CNRS - Ecole Normale Supérieure)

* **TernGrad: Ternary Gradients to Reduce Communication in Distributed Deep Learning**
Wei Wen (Duke University) · Cong Xu (Hewlett Packard Labs) · Feng Yan (University of Nevada, Reno) · Chunpeng Wu (Duke University) · Yandan Wang (University of Pittsburgh) · Yiran Chen (Duke University) · Hai Li (Duke University)

* Cost efficient gradient boosting
Sven Peter (University Heidelberg) · Ferran Diego () · Fred Hamprecht (Heidelberg University) · Boaz Nadler (Weizmann Institute of Science)

* Online to Offline Conversions and Adaptive Minibatch Sizes
Kfir Levy (ETH)

* **Communication-Efficient Stochastic Gradient Descent, with Applications to Neural Networks**
Dan Alistarh (IST Austria & ETH Zurich) · Demjan Grubic (ETH Zurich / Google) · Jerry Li (MIT) · Ryota Tomioka (Microsoft Research Cambridge) · Milan Vojnovic (London School of Economics and Political Science (LSE))

* **Train longer, generalize better: closing the generalization gap in large batch training of neural networks**
Elad Hoffer (Technion) · Itay Hubara (Technion) · Daniel Soudry (Technion)

* Flexpoint: An Adaptive Numerical Format for Efficient Training of Deep Neural Networks
Arjun K Bansal (Intel Nervana) · William Constable (Intel) · Oguz Elibol (Intel Nervana) · Stewart Hall (Intel) · Luke Hornof (Intel) · Amir Khosrowshahi (Intel) · Carey Kloss (Intel) · Urs Köster (Intel Corporation) · Marcel Nassar (Intel Corporation) · Naveen Rao (Intel) · Xin Wang (Intel Corporation) · Tristan Webb (Intel / Nervana)

* Batch Renormalization: Towards Reducing Minibatch Dependence in Batch-Normalized Models
Sergey Ioffe (Google)

* **First-Order Adaptive Sample Size Methods to Reduce Complexity of Empirical Risk Minimization**
Aryan Mokhtari (University of Pennsylvania) · Alejandro Ribeiro (University of Pennsylvania)

* Backprop without Learning Rates Through Coin Betting
Francesco Orabona (Stony Brook University) · Tatiana Tommasi (University of Rome La Sapienza)

* Compressing the Gram Matrix for Learning Neural Networks in Polynomial Time
Surbhi Goel (University of Texas at Austin) · Adam Klivans (UT Austin)

* Non-convex Finite-Sum Optimization Via SCSG Methods
Lihua Lei (UC Berkeley) · Cheng Ju (University of California, Berkeley) · Jianbo Chen (University of California, Berkeley) · Michael Jordan (UC Berkeley)

* Inner-loop free ADMM using Auxiliary Deep Neural Networks
Kai Fan (Duke University) · Qi Wei (Duke University) · Katherine A Heller (Duke)

* Scalable Demand-Aware Recommendation
Jinfeng Yi (IBM Thomas J. Watson Research Center) · Cho-Jui Hsieh (UC Davis) · Kush R Varshney (IBM Research) · Lijun Zhang (Nanjing University (NJU)) · Yao Li (University of California, Davis)

* Nonlinear random matrix theory for deep learning
Jeffrey Pennington (Google Brain) · Pratik Worah (Google)

* An Empirical Bayes Approach to Optimizing Machine Learning Algorithms
James McInerney (Spotify Research)

* A Bayesian Data Augmentation Approach for Learning Deep Models
Toan Tran (The University of Adelaide) · Trung T Pham (The University of Adelaide) · Gustavo Carneiro (The University of Adelaide) · Lyle Palmer (The University of Adelaide) · Ian Reid (University of Adelaide)

* A Highly Efficient Gradient Boosting Decision Tree
Guolin Ke (Microsoft Research) · Qi Meng (Peking University) · Taifeng Wang (Microsoft Research) · Wei Chen (Microsoft Research) · Weidong Ma (Microsoft Research) · Tie-Yan Liu (Microsoft Research)

* Sparse k-Means Embedding
Weiwei Liu (UTS) · Xiaobo Shen (NJUST) · Ivor Tsang (University of Technology, Sydney)

* Adaptive Batch Size for Safe Policy Gradients  (note: 强化学习)
Matteo Papini (Politecnico di Milano) · Matteo Pirotta (INRIA Lille-Nord Europe) · Marcello Restelli ()

* Non-parametric Neural Networks
Andreas Lehrmann (Disney Research) · Leonid Sigal (Disney Research / University of British Columbia)

* Robust Optimization for Non-Convex Objectives
Yaron Singer (Harvard University) · Robert S Chen (Harvard University) · Vasilis Syrgkanis (Microsoft Research) · Brendan Lucier (Microsoft Research)

* Learning to Prune Deep Neural Networks via Layer-wise Optimal Brain Surgeon
Xin Dong (Nanyang Technological Univ) · Shangyu Chen (Nanyang Technological Unvi) · Sinno Pan (NTU)

* **Can Decentralized Algorithms Outperform Centralized Algorithms? A Case Study for Decentralized Parallel Stochastic Gradient Descent**
Xiangru Lian (University of Rochester) · Ce Zhang (ETH Zurich) · Huan Zhang () · Cho-Jui Hsieh (UC Davis) · Wei Zhang (IBM T.J.Watson Research Center) · Ji Liu (University of Rochester)

* Thinking Fast and Slow with Deep Learning and Tree Search
Thomas Anthony (UCL) · Zheng Tian (UCL) · David Barber (University College London)

* On the Complexity of Learning Neural Networks
Le Song (Georgia Institute of Technology) · Santosh Vempala (Georgia Tech) · John Wilmes (Georgia Institute of Technology) · Bo Xie (Georgia Tech)

* Exploring Generalization in Deep Learning
Behnam Neyshabur (Institute for Advanced Study) · Srinadh Bhojanapalli (Toyota Technological Institute at Chicago) · Nati Srebro (TTI-Chicago)

* Asynchronous Coordinate Descent under More Realistic Assumptions
Tao Sun (National university of defense technology) · Robert Hannah (UCLA) · Wotao Yin (University of California, Los Angeles)

* Stochastic Mirror Descent for Non-Convex Optimization
Zhengyuan Zhou (Stanford University) · Panayotis Mertikopoulos (CNRS (French National Center for Scientific Research)) · Nicholas Bambos () · Stephen Boyd (Stanford University) · Peter W Glynn (Stanford University)

* Straggler Mitigation in Distributed Optimization Through Data Encoding  (note: 缓解拖后腿现象)
Can Karakus (UCLA) · Yifan Sun () · Suhas Diggavi (UCLA) · Wotao Yin (University of California, Los Angeles)

