NIPS 2017

* Breaking the Nonsmooth Barrier: A Scalable Parallel Method for Composite Optimization

Fabian Pedregosa (UC Berkeley / ETH Zurich) · Rémi Leblond (INRIA) · Simon Lacoste-Julien (Université de Montréal)

* Parametric Simplex Method for Sparse Learning

Haotian Pang (Princeton University) · Tuo Zhao (Georgia Tech) · Han Liu (Tencent AI Lab) · Robert J Vanderbei (Princeton University)

* Dykstra's Algorithm, ADMM, and Coordinate Descent: Connections, Insights, and Extensions

Ryan Tibshirani (Carnegie Mellon University)

* Coded Distributed Computing for Inverse Problems

Yaoqing Yang (Carnegie Mellon University) · Pulkit Grover (CMU) · Soummya Kar (Carnegie Mellon University)

* Active Bias: Training a More Accurate Neural Network by Emphasizing High Variance Samples

Haw-Shiuan Chang (UMass, Amherst) · Andrew McCallum (UMass Amherst) · Erik Learned-Miller (UMass Amherst)

* Gradient Descent Can Take Exponential Time to Escape Saddle Points

Simon Du (Carnegie Mellon University) · Chi Jin (UC Berkeley) · Jason D Lee (USC) · Michael Jordan (UC Berkeley) · Aarti Singh (CMU) · Barnabas Poczos (Carnegie Mellon University)

* Integration Methods and Optimization Algorithms

Damien Scieur (INRIA - ENS) · Vincent Roulet (INRIA / ENS Ulm) · Francis Bach (Inria) · Alexandre d'Aspremont (CNRS - Ecole Normale Supérieure)

* Sharpness, Restart and Acceleration

Vincent Roulet (INRIA / ENS Ulm) · Alexandre d'Aspremont (CNRS - Ecole Normale Supérieure)

* **TernGrad: Ternary Gradients to Reduce Communication in Distributed Deep Learning**
Wei Wen (Duke University) · Cong Xu (Hewlett Packard Labs) · Feng Yan (University of Nevada, Reno) · Chunpeng Wu (Duke University) · Yandan Wang (University of Pittsburgh) · Yiran Chen (Duke University) · Hai Li (Duke University)

* Cost efficient gradient boosting

Sven Peter (University Heidelberg) · Ferran Diego () · Fred Hamprecht (Heidelberg University) · Boaz Nadler (Weizmann Institute of Science)

* Online to Offline Conversions and Adaptive Minibatch Sizes

Kfir Levy (ETH)

* **Communication-Efficient Stochastic Gradient Descent, with Applications to Neural Networks**

Dan Alistarh (IST Austria & ETH Zurich) · Demjan Grubic (ETH Zurich / Google) · Jerry Li (MIT) · Ryota Tomioka (Microsoft Research Cambridge) · Milan Vojnovic (London School of Economics and Political Science (LSE))

* **Train longer, generalize better: closing the generalization gap in large batch training of neural networks**

Elad Hoffer (Technion) · Itay Hubara (Technion) · Daniel Soudry (Technion)

* **First-Order Adaptive Sample Size Methods to Reduce Complexity of Empirical Risk Minimization**

Aryan Mokhtari (University of Pennsylvania) · Alejandro Ribeiro (University of Pennsylvania)

* Backprop without Learning Rates Through Coin Betting

Francesco Orabona (Stony Brook University) · Tatiana Tommasi (University of Rome La Sapienza)

* Non-convex Finite-Sum Optimization Via SCSG Methods

Lihua Lei (UC Berkeley) · Cheng Ju (University of California, Berkeley) · Jianbo Chen (University of California, Berkeley) · Michael Jordan (UC Berkeley)

* An Empirical Bayes Approach to Optimizing Machine Learning Algorithms

James McInerney (Spotify Research)

* A Highly Efficient Gradient Boosting Decision Tree

Guolin Ke (Microsoft Research) · Qi Meng (Peking University) · Taifeng Wang (Microsoft Research) · Wei Chen (Microsoft Research) · Weidong Ma (Microsoft Research) · Tie-Yan Liu (Microsoft Research)

* Adaptive Batch Size for Safe Policy Gradients  (note: 强化学习)

Matteo Papini (Politecnico di Milano) · Matteo Pirotta (INRIA Lille-Nord Europe) · Marcello Restelli ()

* Robust Optimization for Non-Convex Objectives

Yaron Singer (Harvard University) · Robert S Chen (Harvard University) · Vasilis Syrgkanis (Microsoft Research) · Brendan Lucier (Microsoft Research)

* **Can Decentralized Algorithms Outperform Centralized Algorithms? A Case Study for Decentralized Parallel Stochastic Gradient Descent**

Xiangru Lian (University of Rochester) · Ce Zhang (ETH Zurich) · Huan Zhang () · Cho-Jui Hsieh (UC Davis) · Wei Zhang (IBM T.J.Watson Research Center) · Ji Liu (University of Rochester)

* Thinking Fast and Slow with Deep Learning and Tree Search

Thomas Anthony (UCL) · Zheng Tian (UCL) · David Barber (University College London)

* Exploring Generalization in Deep Learning

Behnam Neyshabur (Institute for Advanced Study) · Srinadh Bhojanapalli (Toyota Technological Institute at Chicago) · Nati Srebro (TTI-Chicago)

* Straggler Mitigation in Distributed Optimization Through Data Encoding  (note: 缓解拖后腿现象)

Can Karakus (UCLA) · Yifan Sun () · Suhas Diggavi (UCLA) · Wotao Yin (University of California, Los Angeles)

