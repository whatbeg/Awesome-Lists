NIPS 2016

* A Multi-Batch L-BFGS Method for Machine Learning

Albert S Berahas · Jorge Nocedal · Martin Takac

* Proximal Stochastic Methods for Nonsmooth Nonconvex Finite-Sum Optimization

Sashank J. Reddi · Suvrit Sra · Barnabas Poczos · Alexander J Smola

* Sparse Support Recovery with Non-smooth Loss Functions

Kévin Degraux · Gabriel Peyré · Jalal Fadili · Laurent Jacques

* Learning the Number of Neurons in Deep Networks

Jose M Alvarez · Mathieu Salzmann

* On Explore-Then-Commit strategies

Aurelien Garivier · Tor Lattimore · Emilie Kaufmann

* Maximal Sparsity with Deep Networks?

Bo Xin · Yizhou Wang · Wen Gao · Baoyuan Wang · David Wipf

* **Stochastic Gradient MCMC with Stale Gradients**

Changyou Chen · Nan Ding · Chunyuan Li · Yizhe Zhang · Lawrence Carin

* The Power of Optimization from Samples

Eric Balkanski · Aviad Rubinstein · Yaron Singer

* Memory-Efficient Backpropagation Through Time

Audrunas Gruslys · Remi Munos · Ivo Danihelka · Marc Lanctot · Alex Graves

* Stochastic Variance Reduction Methods for Saddle-Point Problems

Balamurugan Palaniappan · Francis Bach

* The Generalized Reparameterization Gradient

Francisco R Ruiz · Michalis Titsias RC AUEB · David Blei

* **Learning Structured Sparsity in Deep Neural Networks**

Wei Wen · Chunpeng Wu · Yandan Wang · Yiran Chen · Hai Li

* Optimal Architectures in a Solvable Model of Deep Networks

Jonathan Kadmon · Haim Sompolinsky

* Scalable Adaptive Stochastic Optimization Using Random Projections

Gabriel Krummenacher · Brian McWilliams · Yannic Kilcher · Joachim M Buhmann · Nicolai Meinshausen

* **A Probabilistic Framework for Deep Learning**

Ankit B Patel · Minh Tan Nguyen · Richard Baraniuk

* **Asynchronous Parallel Greedy Coordinate Descent**

Yang You · Xiangru Lian · Ji Liu · Hsiang-Fu Yu · Inderjit S Dhillon · James Demmel · Cho-Jui Hsieh

* Adaptive Averaging in Accelerated Descent Dynamics

Walid Krichene · Alexandre Bayen · Peter L Bartlett

* **Swapout: Learning an ensemble of deep architectures**

Saurabh Singh · Derek Hoiem · David Forsyth

* Accelerating Stochastic Composition Optimization

Mengdi Wang · Ji Liu · Ethan Fang

* Toward Deeper Understanding of Neural Networks: The Power of Initialization and a Dual View on Expressivity

Amit Daniely · Roy Frostig · Yoram Singer

* Without-Replacement Sampling for Stochastic Gradient Methods

Ohad Shamir

* **Deep Learning without Poor Local Minima**

Kenji Kawaguchi

* Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks

Tim Salimans · Diederik P Kingma

* Showing versus doing: Teaching by demonstration

Mark K Ho · Michael Littman · James MacGlashan · Fiery Cushman · Joe Austerweil · Joseph L Austerweil
